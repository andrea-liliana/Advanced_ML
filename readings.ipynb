{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = \"\"\"AM Saxe et al, \"On the Information Bottleneck Theory of Deep Learning\", 2018.\n",
    "LG Valiant, \"A theory of the learnable\", 1984.\n",
    "Breiman, \"Statistical modeling: the Two Cultures\", 2001.\n",
    "Chen et al, \"Neural Ordinary Differential Equations\", 2018.\n",
    "Xie et al, \"Self-training with Noisy Student improves ImageNet classification\", 2019.\n",
    "Sturmfels et al, \"Visualizing the Impact of Feature Attribution Baselines\", 2020.\n",
    "Carter et al, \"Exploring Neural Networks with Activation Atlases\", 2019.\n",
    "Schölkopf et al., \"Towards causal representation learning\", 2021.\n",
    "Nakkiran et al., \"Deep Double Descent: Where Bigger Models and More Data Hurt\", 2019\n",
    "Hüllermeier and Waegeman, \"Aleatoric and Epistemic Uncertainty in Machine Learning: An Introduction to Concepts and Methods\", 2020.\n",
    "Rudin, \"Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead\", 2019.\n",
    "Stewart and Ermon, \"Label-Free Supervision of Neural Networks with Physics and Domain Knowledge\", 2016.\n",
    "Brown et al, \"Language Models are Few-Shot Learners\", 2020.\n",
    "Durkan et al, \"Neural spline flows\", 2019.\n",
    "Sitzmann et al, \"Implicit Neural Representations with Periodic Activation Functions\", 2020.\n",
    "Zhao et al, \"Learning Hierarchical Features from Deep Generative Models\", 2017.\n",
    "Radford et al, \"GPT-2: Language Models are Unsupervised Multitask Learners\", 2019.\n",
    "Ilyas et al, \"Adversarial Examples Are Not Bugs, They Are Features\", 2019.\n",
    "Schrittwieser et al, \"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model\", 2019.\n",
    "Zhang et al, \"mixup: Beyond Empirical Risk Minimization, 2017.\n",
    "Ha and Schmidhuber, \"Recurrent World Models Facilitate Policy Evolution\", 2018.\n",
    "Tan et al, \"Efficientnet: Rethinking model scaling for convolutional neural networks\", 2019.\n",
    "Kilbertus et al, \"Avoiding discrimination through causal reasoning\", 2017.\n",
    "Neyshabur, \"The role of over-parametrization in generalization of neural networks\", 2019.\n",
    "Petar Veličković et al, \"Graph Attention Networks\", 2018.\n",
    "Adabi et al, \"Deep learning with differential privacy\", 2016.\n",
    "Parisi et al, \"Continual lifelong learning with neural networks: a review\", 2019.\"\"\"\n",
    "papers = papers.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups = \"\"\"Baptiste Debes, Rémi Raimondi, Pierre-François Weyders\n",
    "Louis Marganne, Adrien Schoffeniels, Valentin Vermeylen\n",
    "Florent De Geeter, Thomas Pirenne, Corinne Schleich\n",
    "Ba Le, Bastien Pinchard, Dominik Zians\n",
    "Jean-Philippe Cabay, Jan Held, Christal Mangolopa\n",
    "Benoit Thielen, Hannotte Alexandre, Thibaut Doxins\n",
    "María Andrea Liliana Gómez Herrera, Chabane Guillaume Otmani, Nicolas Radis\n",
    "Luca Backes, Ludo Bissot, Martin Laurent\n",
    "Charles Baudinet, Victor Dachet, Guillaume Langer\n",
    "David Birtles, Adrien Guilliams\n",
    "Nicolas Christiaens, Matthias Pirlet, Gabriele Mosaico\n",
    "Gaël Di Raimo, Thomas Lucas, Thibault Malay\n",
    "Pierre Dumoulin, Maxence gilson\n",
    "Stephan Defraire, Thomas Mazur, Nicolas Tilkin\n",
    "Nicolas Dessambre, Andreas Duquenne, Nicolas Fares\n",
    "Ruslan Alagov, Aleksei Korneev\"\"\"\n",
    "groups = groups.split(\"\\n\")\n",
    "sum([len(g.split(\",\")) for g in groups])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Nakkiran et al., \"Deep Double Descent: Where Bigger Models and More Data Hurt\", 2019\n",
      "--> Baptiste Debes, Rémi Raimondi, Pierre-François Weyders\n",
      "\n",
      "- Zhang et al, \"mixup: Beyond Empirical Risk Minimization, 2017.\n",
      "--> Louis Marganne, Adrien Schoffeniels, Valentin Vermeylen\n",
      "\n",
      "- LG Valiant, \"A theory of the learnable\", 1984.\n",
      "--> Florent De Geeter, Thomas Pirenne, Corinne Schleich\n",
      "\n",
      "- Stewart and Ermon, \"Label-Free Supervision of Neural Networks with Physics and Domain Knowledge\", 2016.\n",
      "--> Ba Le, Bastien Pinchard, Dominik Zians\n",
      "\n",
      "- Zhao et al, \"Learning Hierarchical Features from Deep Generative Models\", 2017.\n",
      "--> Jean-Philippe Cabay, Jan Held, Christal Mangolopa\n",
      "\n",
      "- Schölkopf et al., \"Towards causal representation learning\", 2021.\n",
      "--> Benoit Thielen, Hannotte Alexandre, Thibaut Doxins\n",
      "\n",
      "- Neyshabur, \"The role of over-parametrization in generalization of neural networks\", 2019.\n",
      "--> María Andrea Liliana Gómez Herrera, Chabane Guillaume Otmani, Nicolas Radis\n",
      "\n",
      "- Parisi et al, \"Continual lifelong learning with neural networks: a review\", 2019.\n",
      "--> Luca Backes, Ludo Bissot, Martin Laurent\n",
      "\n",
      "- AM Saxe et al, \"On the Information Bottleneck Theory of Deep Learning\", 2018.\n",
      "--> Charles Baudinet, Victor Dachet, Guillaume Langer\n",
      "\n",
      "- Ilyas et al, \"Adversarial Examples Are Not Bugs, They Are Features\", 2019.\n",
      "--> David Birtles, Adrien Guilliams\n",
      "\n",
      "- Xie et al, \"Self-training with Noisy Student improves ImageNet classification\", 2019.\n",
      "--> Nicolas Christiaens, Matthias Pirlet, Gabriele Mosaico\n",
      "\n",
      "- Hüllermeier and Waegeman, \"Aleatoric and Epistemic Uncertainty in Machine Learning: An Introduction to Concepts and Methods\", 2020.\n",
      "--> Gaël Di Raimo, Thomas Lucas, Thibault Malay\n",
      "\n",
      "- Petar Veličković et al, \"Graph Attention Networks\", 2018.\n",
      "--> Pierre Dumoulin, Maxence gilson\n",
      "\n",
      "- Durkan et al, \"Neural spline flows\", 2019.\n",
      "--> Stephan Defraire, Thomas Mazur, Nicolas Tilkin\n",
      "\n",
      "- Sturmfels et al, \"Visualizing the Impact of Feature Attribution Baselines\", 2020.\n",
      "--> Nicolas Dessambre, Andreas Duquenne, Nicolas Fares\n",
      "\n",
      "- Kilbertus et al, \"Avoiding discrimination through causal reasoning\", 2017.\n",
      "--> Ruslan Alagov, Aleksei Korneev\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(13)\n",
    "\n",
    "permutation = np.random.permutation(len(papers))\n",
    "for g, t in zip(groups, permutation):\n",
    "    print(\"-\", papers[t])\n",
    "    print(\"-->\", g) \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Sitzmann et al, \"Implicit Neural Representations with Periodic Activation Functions\", 2020.\n",
      "- Tan et al, \"Efficientnet: Rethinking model scaling for convolutional neural networks\", 2019.\n",
      "- Chen et al, \"Neural Ordinary Differential Equations\", 2018.\n",
      "- Brown et al, \"Language Models are Few-Shot Learners\", 2020.\n",
      "- Ha and Schmidhuber, \"Recurrent World Models Facilitate Policy Evolution\", 2018.\n",
      "- Breiman, \"Statistical modeling: the Two Cultures\", 2001.\n",
      "- Carter et al, \"Exploring Neural Networks with Activation Atlases\", 2019.\n",
      "- Adabi et al, \"Deep learning with differential privacy\", 2016.\n",
      "- Rudin, \"Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead\", 2019.\n",
      "- Radford et al, \"GPT-2: Language Models are Unsupervised Multitask Learners\", 2019.\n",
      "- Schrittwieser et al, \"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model\", 2019.\n"
     ]
    }
   ],
   "source": [
    "# Unassigned papers\n",
    "for t in permutation[len(groups):]:\n",
    "    print(\"-\", papers[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swap\n",
    "\n",
    "1. Jean-Philippe Cabay, Jan Held, Christal Mangolopa swap \"Learning Hierarchical Features from Deep Generative Models\" (now unassigned) for \"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model\".\n",
    "2. Florent De Geeter, Thomas Pirenne, Corinne Schleich swap \"A theory of the learnable\" (now unassigned) for \"Recurrent World Models Facilitate Policy Evolution\".\n",
    "3. Baptiste Debes, Rémi Raimondi, Pierre-François Weyders swap \"Deep double descent where bigger models and more data hurts\" (now unassigned) for \"Efficientnet: Rethinking model scaling for convolutional neural networks\".\n",
    "4. Charles Baudinet, Victor Dachet, Guillaume Langer swap \"On the Information Bottleneck Theory of Deep Learning\" (now unassigned) for \"Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead\".\n",
    "5. Benoit Thielen, Hannotte Alexandre, Thibaut Doxins swap \"Towards causal representation learning\" (now unassigned) for \"GPT-2: Language Models are Unsupervised Multitask Learners\".\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
